# This is a project of different types of optimizers

#### This prpoject takes a toy dataset and applies different Optimizers to converge the model

#### A graph is plotted of cost function against hyper-parameter to show the effects of different optimizers on learning rate of the model

#### There are 8 different Optimzers used and each of them is written in a seperated Function

1)Batch Gradient Descent

2)Stochastic Gradient Descent

3)Mini-Batch Gradient Descent

4)Momentum based Gradient Descent

5)Nestrov Accelerated Gradient Descent

6)Adagrad Descent

7)Adadelta Descent

8)Adam Descet   
