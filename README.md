# Comparative Analysis of Gradient Descent Optimizers


In this project, different optimizers for gradient descent are developed from scratch and applied to a toy dataset to achieve model convergence. Moreover, a graph is plotted to demonstrate the influence of these optimizers on the model's learning rate by mapping the cost function against parameters.

#### Following 8 Different Optimizers are used.

1. Batch Gradient Descent

2. Stochastic Gradient Descent

3. Mini-Batch Gradient Descent

4. Momentum based Gradient Descent

5. Nestrov Accelerated Gradient Descent

6. Adagrad Descent

7. Adadelta Descent

8. Adam Descet   

\
The Jupyter notebook contains the code as well as the plotted graphs, and a research survey paper has been created to present the results of the optimization techniques comparison.

